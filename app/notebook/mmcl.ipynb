{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aae95c-7c4f-4cb6-836b-d19dffa9b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c290b58-aeb9-4179-b6a7-e1e39e33c3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.lawati/mol-incontext/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import load_dataset\n",
    "from util.model import smiles2graph\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "from model.mmcl import MultiModalCLGAE, train as train_mmcl\n",
    "from util.dataset import GraphTextDataset\n",
    "from util.scibert import get_batched_text_outputs, get_tokenizer\n",
    "\n",
    "from itertools import chain\n",
    "from util.prompt import create_cot_prompt, create_incontext_prompt2\n",
    "from util.measure import measure\n",
    "import torch_geometric.utils.smiles as smiles\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c4d6ac-6382-4b93-87e3-14f9d3e9abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'liupf/ChEBI-20-MM'\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "df_train = dataset['train'].to_pandas()\n",
    "df_valid = dataset['validation'].to_pandas()\n",
    "df_test = dataset['test'].to_pandas()\n",
    "\n",
    "def smiles2graph(smiles_str):\n",
    "    data = smiles.from_smiles(smiles_str)\n",
    "    data.edge_attr = data.edge_attr.float()\n",
    "    data.x = data.x.float()\n",
    "    return Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f821fd01-e276-40c4-82d5-2cdd8ee6a061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CID                                                    129626631\n",
       "SMILES         CCCCC[C@@H]1O[C@@H]1/C=C/C(O)C/C=C\\C/C=C\\CCCC(...\n",
       "description    The molecule is an epoxy(hydroxy)icosatrienoat...\n",
       "polararea                                                   72.9\n",
       "xlogp                                                        4.6\n",
       "inchi          InChI=1S/C20H32O4/c1-2-3-9-13-18-19(24-18)16-1...\n",
       "iupacname      (5Z,8Z,12E)-11-hydroxy-13-[(2R,3S)-3-pentyloxi...\n",
       "SELFIES        [C][C][C][C][C][C@@H1][O][C@@H1][Ring1][Ring1]...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb7b042-5284-4039-a59e-35eb95c2db89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[24, 9], edge_index=[2, 48], edge_attr=[48, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles2graph(df_train.iloc[0]['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa82d77b-1bc6-4a9d-9b68-c75882b502dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 512\n",
    "batch_size = 256\n",
    "text_tokenizer, text_model = get_tokenizer()\n",
    "\n",
    "df_train = df_train[0:1000]\n",
    "df_valid = df_valid[0:500]\n",
    "df_test = df_test[0:500]\n",
    "\n",
    "train_graphs = [smiles2graph(smiles) for smiles in df_train['SMILES']]\n",
    "val_graphs = [smiles2graph(smiles) for smiles in df_valid['SMILES']]\n",
    "test_graphs = [smiles2graph(smiles) for smiles in df_test['SMILES']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa16fd4-a052-4b2e-a089-4c4ec8c3a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025010/1131015796.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/ali.lawati/mol-incontext/checkpoints/mmcl-300.pt', map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.mmcl_attr import MultiModalCLAttr\n",
    "\n",
    "model = MultiModalCLAttr(9, 32, 64, 9)  # Replace with your model class\n",
    "model.load_state_dict(torch.load('/home/ali.lawati/mol-incontext/checkpoints/mmcl-300.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4851c5-bfe1-4edc-9139-2bd4c84c9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    train_batch = Batch.from_data_list(train_graphs)#.to(device)\n",
    "    test_batch  = Batch.from_data_list(test_graphs)#.to(device)\n",
    "    train_pool = model(train_batch.x, train_batch.edge_index, train_batch.batch, train_batch.edge_attr)\n",
    "    test_pool = model(test_batch.x, test_batch.edge_index, test_batch.batch, test_batch.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7902cd75-0270-4182-9891-ad45833836c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = model(train_batch.x, train_batch.edge_index, train_batch.batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ba64a53-a8a4-40fc-ba08-fcc979657d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pool.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae090e8-1cd1-46de-83ad-6ebfb3aa216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.text2latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34596d-d3ec-4753-9814-64558cded93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "533ba1be-daa0-41de-8a36-de837e7a78f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    " \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument(\"--seed\", type=int, default=42)\n",
    "parser.add_argument(\"--device\", type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--dataspace_path\", type=str, default=\"./data\")\n",
    "parser.add_argument(\"--SSL_emb_dim\", type=int, default=256)\n",
    "parser.add_argument(\"--max_seq_len\", type=int, default=512)\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "pretrained_SciBERT_folder = os.path.join(args.dataspace_path, 'pretrained_SciBERT')\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder)\n",
    "text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=pretrained_SciBERT_folder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1851af7-5c47-4fc1-bee6-be2b75254232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for BERT\n",
    "def padarray(A, size, value=0):\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n",
    "\n",
    "def preprocess_each_sentence(sentence, tokenizer, max_seq_len):\n",
    "    text_input = tokenizer(\n",
    "        sentence, truncation=True, max_length=max_seq_len,\n",
    "        padding='max_length', return_tensors='np')\n",
    "    \n",
    "    input_ids = text_input['input_ids'].squeeze()\n",
    "    attention_mask = text_input['attention_mask'].squeeze()\n",
    "\n",
    "    sentence_tokens_ids = padarray(input_ids, max_seq_len)\n",
    "    sentence_masks = padarray(attention_mask, max_seq_len)\n",
    "    return [sentence_tokens_ids, sentence_masks]\n",
    "\n",
    "\n",
    "# This is for BERT\n",
    "def prepare_text_tokens(device, description, tokenizer, max_seq_len):\n",
    "    B = len(description)\n",
    "    tokens_outputs = [preprocess_each_sentence(description[idx], tokenizer, max_seq_len) for idx in range(B)]\n",
    "    tokens_ids = [o[0] for o in tokens_outputs]\n",
    "    masks = [o[1] for o in tokens_outputs]\n",
    "    tokens_ids = torch.Tensor(tokens_ids).long().to(device)\n",
    "    masks = torch.Tensor(masks).bool().to(device)\n",
    "    return tokens_ids, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f5c5d9b-63dd-4746-8a3a-24d52966347a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m description_repr \u001b[38;5;241m=\u001b[39m description_output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooler_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m text_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m768\u001b[39m\n\u001b[0;32m----> 9\u001b[0m description_repr \u001b[38;5;241m=\u001b[39m \u001b[43mtext2latent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe molecule is an 11-oxo steroid that is corticosterone in which the hydroxy substituent at the 11beta position has been oxidised to give the corresponding ketone. It has a role as a human metabolite and a mouse metabolite. It is a 21-hydroxy steroid, a 3-oxo-Delta(4) steroid, a 20-oxo steroid, an 11-oxo steroid, a corticosteroid and a primary alpha-hydroxy ketone. It derives from a corticosterone.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mol-incontext/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mol-incontext/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mol-incontext/env/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "text_dim = 768\n",
    "text2latent = torch.nn.Linear(text_dim, args.SSL_emb_dim).to(device)\n",
    "\n",
    "description_tokens_ids, description_masks = prepare_text_tokens(device, ['The molecule is a branched amino tetrasaccharide consisting of N-acetyl-beta-D-glucosamine having two alpha-L-fucosyl residues at the 3- and 6-positions as well as an N-acetyl-beta-D-glucosaminyl residue at the 4-position. It has a role as a carbohydrate allergen. It is a glucosamine oligosaccharide and an amino tetrasaccharide. It derives from an alpha-L-Fucp-(1->3)-[alpha-L-Fucp-(1->6)]-beta-D-GlcpNAc'],text_tokenizer, 500) \n",
    "\n",
    "description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
    "description_repr = description_output[\"pooler_output\"]\n",
    "description_repr = text2latent(\"The molecule is an 11-oxo steroid that is corticosterone in which the hydroxy substituent at the 11beta position has been oxidised to give the corresponding ketone. It has a role as a human metabolite and a mouse metabolite. It is a 21-hydroxy steroid, a 3-oxo-Delta(4) steroid, a 20-oxo steroid, an 11-oxo steroid, a corticosteroid and a primary alpha-hydroxy ketone. It derives from a corticosterone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f5445-f3e0-4ccf-98f4-a4501065c770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
