{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f33be56c-192f-48db-9284-1352670ff19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from datasets import load_dataset\n",
    "from train import smiles2graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6cdebcbd-4762-4319-96a1-91e87475b1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3206895/1373988659.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/ali.lawati/mol-incontext/checkpoints/mmcl-300.pt', map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from torch_geometric.data import Data, Batch\n",
    "from model.mmcl_attr import MultiModalCLAttr\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from model.airl3 import AIRL\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "cache_dir = '/home/ali.lawati/mol-incontext/data/pretrained_SciBERT'\n",
    "text_tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=cache_dir)\n",
    "text_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', cache_dir=cache_dir).to(device)\n",
    "\n",
    "def padarray(A, size, value=0):\n",
    "    t = size - len(A)\n",
    "    return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n",
    "\n",
    "def preprocess_each_sentence(sentence, tokenizer, max_seq_len):\n",
    "    text_input = tokenizer(\n",
    "        sentence, truncation=True, max_length=max_seq_len,\n",
    "        padding='max_length', return_tensors='np')\n",
    "    \n",
    "    input_ids = text_input['input_ids'].squeeze()\n",
    "    attention_mask = text_input['attention_mask'].squeeze()\n",
    "\n",
    "    sentence_tokens_ids = padarray(input_ids, max_seq_len)\n",
    "    sentence_masks = padarray(attention_mask, max_seq_len)\n",
    "    return [sentence_tokens_ids, sentence_masks]\n",
    "\n",
    "def embed_text(text2latent, text_model, text_tokenizer, text_arr):\n",
    "    description_tokens_ids, description_masks = prepare_text_tokens(device, text_arr, text_tokenizer, 500) \n",
    "    description_output = text_model(input_ids=description_tokens_ids, attention_mask=description_masks)\n",
    "    description_repr = description_output[\"pooler_output\"]\n",
    "    description_repr = text2latent(description_repr)\n",
    "    return description_repr\n",
    "\n",
    "\n",
    "# This is for BERT\n",
    "def prepare_text_tokens(device, description, tokenizer, max_seq_len):\n",
    "    B = len(description)\n",
    "    tokens_outputs = [preprocess_each_sentence(description[idx], tokenizer, max_seq_len) for idx in range(B)]\n",
    "    tokens_ids = [o[0] for o in tokens_outputs]\n",
    "    masks = [o[1] for o in tokens_outputs]\n",
    "    tokens_ids = torch.Tensor(tokens_ids).long().to(device)\n",
    "    masks = torch.Tensor(masks).bool().to(device)\n",
    "    return tokens_ids, masks\n",
    "\n",
    "def load_local_dataset(dataset_name = 'liupf/ChEBI-20-MM'):\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    df_train = dataset['train'].to_pandas()\n",
    "    df_valid = dataset['validation'].to_pandas()\n",
    "    df_test = dataset['test'].to_pandas()\n",
    "    return df_train, df_valid, df_test\n",
    "\n",
    "def load_model(model_checkpoint = '/home/ali.lawati/mol-incontext/checkpoints/mmcl-300.pt'):\n",
    "    model = MultiModalCLAttr(9, 32, 64, 9)  # Replace with your model class \n",
    "    model.load_state_dict(torch.load('/home/ali.lawati/mol-incontext/checkpoints/mmcl-300.pt', map_location=torch.device('cpu')))\n",
    "    return model, model.text2latent\n",
    "\n",
    "state_dim = 64\n",
    "action_dim = 64\n",
    "airl = AIRL(state_dim, action_dim)\n",
    "model, text2latent = load_model()\n",
    "df_train, df_valid, df_test = load_local_dataset()\n",
    "\n",
    "val_graphs = [smiles2graph(smiles) for smiles in df_valid['SMILES']]\n",
    "train_graphs = [smiles2graph(smiles) for smiles in df_train['SMILES']]\n",
    "train_batch = Batch.from_data_list(train_graphs).to(device)\n",
    "valid_batch  = Batch.from_data_list(val_graphs).to(device)\n",
    "train_pool = model(train_batch.x, train_batch.edge_index, train_batch.batch, train_batch.edge_attr)\n",
    "valid_pool = model(valid_batch.x, valid_batch.edge_index, valid_batch.batch, valid_batch.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39ed9b83-80ca-41f1-afe9-8fb74f42f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_expert_trajs(text_model, text_tokenizer, text2latent, states_all, actions_all, init_states_all, samples, demos, B=32, ML=2):\n",
    "    \n",
    "    actions_B = actions_all.unsqueeze(0).expand(B, -1, 64)\n",
    "    traj_actions = actions_B[torch.arange(B).unsqueeze(1), demos]\n",
    "\n",
    "    init_states = init_states_all[samples].T.reshape(B,-1)\n",
    "\n",
    "    action_states = states_all[demos]\n",
    "\n",
    "    action_states = np.concatenate((init_states, action_states), axis=1)\n",
    "    action_states = np.char.add.accumulate(np.core.defchararray.add(action_states, ' # '), axis=1)\n",
    "    action_states[:,0] = init_states[:, 0]\n",
    "\n",
    "    embed_states =  torch.tensor(embed_text(text2latent, text_model, text_tokenizer, np.reshape(action_states, -1))).reshape(B,-1, 64)\n",
    "\n",
    "    # Create triplets using tensor indexing\n",
    "    triplet_states = embed_states[:,:-1,:]  # Select all states except the last\n",
    "    triplet_actions = traj_actions       # Actions are the same\n",
    "    triplet_next_states = embed_states[:,1:,:]  # Select all states except the first\n",
    "\n",
    "    # Stack the triplets into a single tensor (optional)\n",
    "    triplets = torch.stack((triplet_states, triplet_actions, triplet_next_states), dim=2)\n",
    "    triplets = triplets.reshape(-1,3,64)\n",
    "    triplets = triplets[torch.multinomial(torch.ones(triplets.shape[0]), B, replacement=False)]\n",
    "    return torch.split(triplets,1, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7a3ea1dd-b73f-4eaf-9a69-d7ec9b061fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_all = torch.cat((train_pool, valid_pool)).detach()\n",
    "init_states_all = np.concatenate((df_valid['SMILES'].values, df_train['SMILES'].values)) # I'm doing this opposite since the prompt files use valid dataset, and so i can index it directly\n",
    "states_all = np.concatenate((df_train['description'].values, df_valid['description'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce01cc97-be50-4dcf-b57c-b543e3297d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low score indices: [0.1 0.2 0.  0.2 0.2 0.1 0.1 0.2 0.2 0.  0.2 0.1 0.2 0.2 0.2 0.2 0.2 0.2\n",
      " 0.2 0.1 0.1 0.1 0.1 0.1 0.2 0.2 0.2 0.2 0.2 0.  0.2 0.1 0.1 0.2 0.1 0.\n",
      " 0.2 0.  0.2 0.2 0.2 0.  0.2 0.2 0.2 0.2 0.  0.2 0.  0.1 0.2 0.1 0.2 0.2\n",
      " 0.2 0.2 0.2 0.2 0.1 0.1 0.1 0.  0.2 0.1 0.1 0.2 0.2 0.2 0.2 0.1 0.2 0.2\n",
      " 0.2 0.  0.2 0.1 0.2 0.  0.1 0.  0.1 0.1 0.2 0.1 0.  0.2 0.2 0.2 0.2 0.1\n",
      " 0.  0.1 0.1 0.1 0.  0.2 0.2 0.1 0.1 0.2 0.1 0.2 0.1 0.2 0.2 0.2 0.  0.2\n",
      " 0.2 0.2 0.2 0.1 0.  0.2 0.1 0.2 0.2 0.1 0.1 0.2 0.1 0.2 0.  0.2 0.1 0.1\n",
      " 0.2 0.2 0.1 0.  0.2 0.2 0.1 0.1 0.2 0.2 0.1 0.2 0.2 0.2 0.2 0.1 0.2 0.2\n",
      " 0.2 0.1 0.1 0.2 0.  0.2 0.  0.1 0.2 0.2 0.  0.  0.1 0.2 0.  0.1 0.2 0.1\n",
      " 0.  0.1 0.1 0.  0.2 0.2 0.1 0.2 0.2 0.2 0.2 0.2 0.  0.  0.2 0.  0.  0.1\n",
      " 0.2 0.2 0.  0.2 0.1 0.2 0.2 0.2 0.2 0.2 0.1 0.2 0.2 0.  0.1 0.  0.1 0.2\n",
      " 0.2 0.  0.  0.2 0.1 0.  0.2 0.1 0.1 0.1 0.1 0.  0.  0.1 0.1 0.2 0.2 0.2\n",
      " 0.2 0.2 0.1 0.2 0.1 0.1 0.1 0.2 0.  0.1 0.2 0.2 0.2 0.1 0.1 0.2 0.2 0.2\n",
      " 0.1 0.  0.2 0.2 0.2 0.2 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.1 0.2 0.1 0.2 0.1\n",
      " 0.2 0.  0.1 0.1 0.2 0.2 0.1 0.1 0.2 0.2 0.  0.2 0.2 0.2 0.1 0.1 0.2 0.1\n",
      " 0.2 0.  0.2 0.1 0.1 0.2 0.1 0.1 0.1 0.2 0.2 0.1 0.2 0.2 0.2 0.2 0.1 0.1\n",
      " 0.  0.1 0.1 0.2 0.2 0.1 0.2 0.1 0.2 0.1 0.2 0.2 0.2 0.2 0.2 0.2 0.1 0.1\n",
      " 0.1 0.1 0.2 0.2 0.2 0.2 0.1 0.2 0.1 0.2 0.2 0.2 0.2 0.  0.2 0.2 0.2 0.1\n",
      " 0.  0.  0.1 0.2 0.2 0.2 0.1 0.2 0.2 0.1 0.2 0.1 0.2 0.  0.2 0.1 0.2 0.2\n",
      " 0.2 0.1 0.1 0.2 0.1 0.1 0.1 0.2 0.  0.2 0.  0.2 0.2 0.1 0.1 0.2 0.1 0.1\n",
      " 0.1 0.2 0.2 0.2 0.2 0.  0.2 0.1 0.1 0.2 0.  0.  0.1 0.1 0.2 0.2 0.1 0.\n",
      " 0.1 0.1 0.1 0.2 0.2 0.2 0.1 0.1 0.1 0.1 0.2 0.1 0.2 0.2 0.2 0.1 0.2 0.1\n",
      " 0.1 0.2 0.1 0.  0.1 0.1 0.1 0.  0.2 0.2 0.2 0.1 0.1 0.2 0.1 0.2 0.1 0.2\n",
      " 0.2 0.1]\n",
      "Corresponding rewards for low scores: [ -4.1  17.2   9.6  -5.6 -14.8  13.9   2.1  -3.6  14.9  12.3  12.2  11.7\n",
      "  13.8   2.3  11.2   7.8  12.1   1.6   2.4  -4.    9.4  15.6  18.1   9.2\n",
      "   7.8  10.3   3.3   2.6   9.1  -4.9  14.9  -5.3  10.4  -6.3  -5.9  14.6\n",
      "  -8.1   1.3   0.8  -5.5  -6.2  -7.5  13.9   4.1   1.8   0.2   6.7   5.4\n",
      "  -7.6   7.1  11.5  -0.6   1.5  -0.2  17.   -6.    3.   -4.6   1.   -2.\n",
      "   2.3   9.2  12.2  -4.3   1.  -12.8  -2.4  13.7  -0.6  12.8  18.1  -1.4\n",
      "  13.3   3.3  14.8   3.2  15.   -5.3   4.   13.8  12.2  -2.3  -4.8  -3.6\n",
      " -10.7   7.5   4.4  15.7  -5.2  14.5   8.7  13.7   1.3   1.4   2.7  -2.1\n",
      "   2.8  17.4   8.3   0.7   4.5  -1.1  -3.6  -8.7  15.1   6.1   7.4  -5.6\n",
      "  12.7  13.2   2.    1.3   9.7  -1.   13.8  -1.4  14.7  12.6   8.5  -2.9\n",
      "  -5.   18.   -2.2   1.8  -1.8  19.7  15.4   7.8  -0.2   6.2  19.1  -3.3\n",
      "  -0.1  17.1  10.9  16.7   9.5   2.2  14.1  -4.3  -1.7  -3.4  -5.9   4.4\n",
      "  15.1  -2.5   9.2  14.5  -3.1  15.5  10.7  -4.2  -9.8   5.4  -4.8  -7.2\n",
      "  -2.1   8.   -4.9  -1.8  13.   14.   -6.5  10.1  14.4  18.3  14.3  -2.7\n",
      "  17.5  14.    0.9  12.1   5.4  -3.6  13.5   6.2   2.9  -4.   16.2  18.\n",
      "  -2.3  14.5  -2.3  13.5  13.   -3.2  12.1  -6.9  -3.2  17.2  16.3  -3.3\n",
      "   1.4  -0.3  -4.2  16.    4.7 -11.3  15.9  -3.9   2.4   2.1   3.7   8.\n",
      "  -5.5  14.    3.8   9.2   2.6  -1.1  -9.5   2.3  -2.2  16.   13.7  -2.6\n",
      "  -3.7  13.9  -2.9  -5.8   7.6  -4.7 -16.6   3.3  14.    2.7   2.9  -1.7\n",
      "   0.8  -4.2  -3.2  -8.3  11.3  -2.9  -3.1  -1.6  -5.5  -0.7  12.5  17.7\n",
      "  13.5   1.3  -3.5  12.8  -1.1   1.2  11.8   0.9   0.5   9.9  17.4  15.7\n",
      "  10.9  -8.3  -3.8  -2.5  15.1  14.4  12.9   0.7   1.1  10.3  -1.8  16.1\n",
      "  10.6  12.9  -6.3  13.4   3.5  12.1  -0.3  -0.3   9.  -17.   -4.2  -5.\n",
      "  -2.9   7.1   4.6  -7.7  11.8  -5.6   7.8  -4.4  -2.2  12.7  -8.4  -3.1\n",
      "   9.7  -3.2 -14.4   5.1   0.6  12.8  -6.5   6.8  -3.3  -5.3   5.1  -1.3\n",
      "  -9.    1.9   8.9   4.5  -3.   -0.6  -4.9  12.9   5.   -1.8  16.2  10.9\n",
      "  -8.3  13.6  13.1  15.2  14.   12.   -8.9   0.2  -6.3  13.6   7.4  -0.6\n",
      "  -4.1   1.   16.4   5.4  17.   -8.3   8.8  -1.   -4.9  -2.7  -7.2   0.3\n",
      "   0.   -1.4  -3.9   2.5  -5.    9.7   2.7   9.6  -4.8   1.9  -1.   -2.8\n",
      "  -5.  -10.7 -10.7  -3.   12.9 -14.   12.9   6.9   1.4  -0.3  -2.2   1.6\n",
      "  -0.2  11.7 -14.8   3.1   9.3  15.4   0.4  17.5  -3.3  17.4   3.   14.3\n",
      "  18.7   2.4  -5.1   5.5  -3.6   0.8  17.   -4.5   5.6   0.1  -7.1  10.2\n",
      "  14.9  11.8  12.2  10.6  -1.9  -5.   -0.9  11.1   1.2   0.9  11.1  14.4\n",
      "  17.7   8.5  -0.6  -4.8  10.5  -4.2  12.6  13.   -2.7  11.5  13.4   8.6\n",
      "  -0.3  -8.9  -0.8  11.3   4.1 -12.5   1.6  -2.4]\n",
      "High score indices: [0.9 0.8 0.8 0.9 0.8 1.  0.8 0.9 0.8 1.  0.9 0.9 0.8 0.9 0.8 0.9 1.  0.8\n",
      " 0.8 0.8 1.  0.9 0.8 0.9 1.  0.8 0.8 0.8 0.9 0.9 0.9 0.8 0.8 0.9 0.9 0.8\n",
      " 0.9 0.9 0.8 0.8 1.  0.8 1.  0.9 0.8 0.8 0.9 0.9 0.8 0.8 1.  0.8 0.8 0.8\n",
      " 1.  0.9 0.8 0.8 1.  0.9 0.9 1.  0.8 0.9 0.8 0.8 0.8 0.9 0.9 0.8 0.9 0.9\n",
      " 1.  0.8 0.9 0.8 0.9 1.  0.9 1.  0.9 0.8 0.9 0.8 0.8 0.9 0.8 0.9 0.8 0.8\n",
      " 0.8 0.8 0.8 0.8 0.8 0.8 0.9 0.9 1.  0.8 1.  0.8 0.8 0.8 0.9 1.  1.  0.9\n",
      " 0.9 0.8 0.8 1.  0.8 0.8 0.9 1.  0.9 0.8 0.8 0.9 0.8 0.8 0.8 0.9 0.8 1.\n",
      " 0.8 0.8 1.  0.8 0.8 0.9]\n",
      "Corresponding rewards for high scores: [  13.  -14.   -6.   -5.   13.   10.   13.  -10.    0.   -9.    5.   12.\n",
      "   -6.   -2.   -1.   -2.    3.   19.   11.   13.   -3.    3.    1.   14.\n",
      "   -4.   14.   -1.   11.   13.   12.   -0.   12.   18.    8.   -2.   -8.\n",
      "   -2.   -2.   -1.    4.   12.    8.   -5.   -6.    4.   17.   10.   -3.\n",
      "   -5.   12.   -9.   -3.    6.    2.   10.    0.    4.   -2.   13.   12.\n",
      "   17.   14.   17.    2.   16.   -6.    0.   -1.    4.   19.   15.    6.\n",
      "    8.   14.   15.    5.   -8.    6.    9.    1.   -0.    4.   -2.   10.\n",
      "   11.   12.   16.    2.   -2.    1.   -0.   13.   -2.   -0.    9.   -1.\n",
      "    7.    3.   -4.    2.    4.    8. -134.   -2.    8.   -4.   16.   11.\n",
      "   -5.   -4.   15.    4.   11.   11.    8.    1.   -1.   11.    8.   11.\n",
      "   10.   18.   -2.    7.   15.   15.    9.   14.    9.   -1.   -0.   -5.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(3.9076923076923076)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "scores = np.round(np.load(f\"/home/ali.lawati/mol-incontext/input/embed/mmcl_attr-chebi-2-epochs300-new-test.mistral-7B.scores.npy\"), 1)\n",
    "rewards = np.round(np.load(\"/home/ali.lawati/mol-incontext/input/embed/mmcl_attr-chebi-2-epochs300-new-test.mistral-7B.rewards.npy\"), 1)\n",
    "\n",
    "# Finding indices of low scores < 0.3\n",
    "low_score_indices = np.where(scores < 0.3)[0]\n",
    "\n",
    "# Finding corresponding rewards for low scores\n",
    "low_score_rewards = rewards[low_score_indices]\n",
    "\n",
    "# Finding indices of high scores > 0.7\n",
    "high_score_indices = np.where(scores > 0.7)[0]\n",
    "\n",
    "# Finding corresponding rewards for high scores\n",
    "high_score_rewards = rewards[high_score_indices]\n",
    "\n",
    "print(\"Low score indices:\", scores[low_score_indices])\n",
    "print(\"Corresponding rewards for low scores:\", low_score_rewards)\n",
    "\n",
    "print(\"High score indices:\", scores[high_score_indices])\n",
    "print(\"Corresponding rewards for high scores:\", np.round(high_score_rewards,0))\n",
    "\n",
    "np.mean(low_score_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d67fe2-091e-49d8-98d8-e41027fd2cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677372a-adf6-4bdd-813c-1f822b14f16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
